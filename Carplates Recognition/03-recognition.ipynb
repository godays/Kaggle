{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "from Levenshtein import distance\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.special import softmax\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as fnn\n",
    "\n",
    "from detection_utils import PlateImageAdjuster, PlateImageExtractor, build_mask, get_rectangular_box\n",
    "from recognition import CRNN, RecognitionDataset, LanguageModel, beam_search\n",
    "from recognition_utils import collate_fn_recognition, decode, normalize_text, Resize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = PlateImageAdjuster()\n",
    "extractor = PlateImageExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare OCR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25633/25633 [39:56<00:00, 10.70it/s]  \n"
     ]
    }
   ],
   "source": [
    "path_data = Path('data')\n",
    "path_ocr_dataset = Path('ocr_data')\n",
    "path_ocr_dataset.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plates_filename = path_data / 'train.json'\n",
    "with open(plates_filename) as f:\n",
    "    json_data = json.load(f)\n",
    "    \n",
    "for sample in tqdm.tqdm(json_data):\n",
    "    if sample['file'] == 'train/25632.bmp':\n",
    "        continue\n",
    "    file_path = path_data / sample['file']\n",
    "    image = cv2.imread(str(file_path))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    for plate in sample['nums']:\n",
    "        box = plate['box']\n",
    "        text = plate['text']\n",
    "        mask = build_mask(box, image)\n",
    "        plate_img = extractor(image, mask, np.array(box))\n",
    "        plate_img = normalizer(plate_img)\n",
    "        text = normalize_text(text)\n",
    "        file_path = path_ocr_dataset / ''.join([text, '.png'])\n",
    "        cv2.imwrite(str(file_path), plate_img)\n",
    "        \n",
    "        # save also bboxes\n",
    "        file_path = path_ocr_dataset / ''.join([text, '_bbox.png'])\n",
    "        raw_box = get_rectangular_box(box)\n",
    "        plate_bbox = image[raw_box[1]:raw_box[3], raw_box[0]:raw_box[2], :]\n",
    "        plate_bbox = normalizer(plate_bbox)\n",
    "        cv2.imwrite(str(file_path), plate_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "crnn = CRNN(rnn_bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn.to(device)\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "optimizer = torch.optim.Adam(crnn.parameters(), lr=3e-4, amsgrad=True, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=1/np.sqrt(10), patience=2,\n",
    "                                                          verbose=True, threshold=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([\n",
    "    Resize(),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                     std=[0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "\n",
    "train_ocr_dataset = RecognitionDataset('ocr_data', transformations, crnn.alphabet, 'train', add_generated=True)\n",
    "val_ocr_dataset = RecognitionDataset('ocr_data', transformations, crnn.alphabet, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_ocr_dataset, \n",
    "                                               batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=num_workers, pin_memory=True, \n",
    "                                               drop_last=False, collate_fn=collate_fn_recognition)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_ocr_dataset, \n",
    "                                             batch_size=1, shuffle=False,\n",
    "                                             num_workers=num_workers, pin_memory=True, \n",
    "                                             drop_last=True, collate_fn=collate_fn_recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'Recognition_model_with_generated_test'\n",
    "writer = SummaryWriter(log_dir=f'tb_logs/{experiment_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [07:45<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 1, 0.8760174544670548\n",
      "Train 1 Levenstein, 2.211356392950881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:48<00:00, 48.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 1, 0.06269977078596248\n",
      "Valid 1 Levenstein, 0.11181434599156118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [06:35<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 2, 0.05210205477577211\n",
      "Train 2 Levenstein, 0.08593457067866517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:52<00:00, 44.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 2, 0.046893362505120745\n",
      "Valid 2 Levenstein, 0.08860759493670886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:38<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 3, 0.03412311075630876\n",
      "Train 3 Levenstein, 0.05865673040869891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:42<00:00, 56.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 3, 0.047217273825221055\n",
      "Valid 3 Levenstein, 0.09113924050632911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:29<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 4, 0.024278267700367537\n",
      "Train 4 Levenstein, 0.042955568053993254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:25<00:00, 93.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 4, 0.042848612364766656\n",
      "Valid 4 Levenstein, 0.07679324894514768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:33<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 5, 0.016438511716002233\n",
      "Train 5 Levenstein, 0.029902512185976754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:25<00:00, 93.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 5, 0.03956691569156005\n",
      "Valid 5 Levenstein, 0.0670886075949367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:32<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 6, 0.012232415945509329\n",
      "Train 6 Levenstein, 0.02462973378327709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:27<00:00, 87.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 6, 0.037110443833617605\n",
      "Valid 6 Levenstein, 0.06497890295358649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:37<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 7, 0.010503782826158842\n",
      "Train 7 Levenstein, 0.021606674165729284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:23<00:00, 100.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 7, 0.03732635374819537\n",
      "Valid 7 Levenstein, 0.0679324894514768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:42<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 8, 0.007906998581594384\n",
      "Train 8 Levenstein, 0.017529058867641546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:24<00:00, 98.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 8, 0.039501472685619564\n",
      "Valid 8 Levenstein, 0.06413502109704641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:43<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 9, 0.005343272933154192\n",
      "Train 9 Levenstein, 0.011553243344581927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:22<00:00, 103.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 9, 0.03856268619063116\n",
      "Valid 9 Levenstein, 0.06666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:41<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 10, 0.0036404026336088286\n",
      "Train 10 Levenstein, 0.00775684289463817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:25<00:00, 92.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 10, 0.0375503298618061\n",
      "Valid 10 Levenstein, 0.05907172995780591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:32<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 11, 0.001943504961934974\n",
      "Train 11 Levenstein, 0.0037495313085864268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:25<00:00, 91.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 11, 0.035227503563875585\n",
      "Valid 11 Levenstein, 0.05780590717299578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:33<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 12, 0.001803808126383394\n",
      "Train 12 Levenstein, 0.0035620547431571056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:24<00:00, 97.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 12, 0.036538671776844826\n",
      "Valid 12 Levenstein, 0.056118143459915615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:33<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 13, 0.003193577966959819\n",
      "Train 13 Levenstein, 0.007006936632920885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:22<00:00, 106.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 13, 0.03624013424636252\n",
      "Valid 13 Levenstein, 0.05822784810126582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:39<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 14, 0.005667311568893534\n",
      "Train 14 Levenstein, 0.013498312710911136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:25<00:00, 92.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 14, 0.04919872837146406\n",
      "Valid 14 Levenstein, 0.08227848101265822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:33<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 15, 0.008320959635805673\n",
      "Train 15 Levenstein, 0.018724221972253468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:23<00:00, 99.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    15: reducing learning rate of group 0 to 9.4868e-05.\n",
      "Valid 15, 0.04338452480025447\n",
      "Valid 15 Levenstein, 0.07383966244725738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:30<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 16, 0.0008811721582123814\n",
      "Train 16 Levenstein, 0.0011482939632545932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:27<00:00, 87.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 16, 0.03433466004446962\n",
      "Valid 16 Levenstein, 0.05063291139240506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:38<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 17, 0.00042470726229053556\n",
      "Train 17 Levenstein, 0.0002109111361079865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:23<00:00, 100.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 17, 0.03477054677993466\n",
      "Valid 17 Levenstein, 0.05021097046413502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:42<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 18, 0.00035168123890096364\n",
      "Train 18 Levenstein, 0.00011717285339332584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:24<00:00, 97.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 18, 0.03533918200871396\n",
      "Valid 18 Levenstein, 0.049367088607594936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:30<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 19, 0.0002930440269178478\n",
      "Train 19 Levenstein, 2.3434570678665168e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:23<00:00, 101.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 19, 0.035319822273530255\n",
      "Valid 19 Levenstein, 0.049367088607594936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [04:26<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 20, 0.0002680685745714151\n",
      "Train 20 Levenstein, 4.6869141357330336e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:22<00:00, 105.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 20, 0.035387376012621326\n",
      "Valid 20 Levenstein, 0.049367088607594936\n"
     ]
    }
   ],
   "source": [
    "best_loss = np.inf\n",
    "prev_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "for i, epoch in enumerate(range(num_epochs)):\n",
    "    epoch_losses = []\n",
    "    levensteint_losses = []\n",
    "    \n",
    "    # Если поменялась lr - загружаем лучшую модель\n",
    "    if optimizer.param_groups[0]['lr'] < prev_lr:\n",
    "        prev_lr = optimizer.param_groups[0]['lr']\n",
    "        with open(f'{experiment_name}.pth', 'rb') as fp:\n",
    "            state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "        crnn.load_state_dict(state_dict)\n",
    "        crnn.to(device)\n",
    "    \n",
    "    crnn.train()\n",
    "    for j, b in enumerate(tqdm.tqdm(train_dataloader, total=len(train_dataloader))):\n",
    "        images = b[\"image\"].to(device)\n",
    "        seqs_gt = b[\"seq\"]\n",
    "        seq_lens_gt = b[\"seq_len\"]\n",
    "\n",
    "        seqs_pred = crnn(images).cpu()\n",
    "        log_probs = fnn.log_softmax(seqs_pred, dim=2)\n",
    "        seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
    "        \n",
    "        texts_pred = decode(seqs_pred, crnn.alphabet)\n",
    "        texts_gt = b[\"text\"]\n",
    "        levensteint_losses.extend([distance(pred, gt) for pred, gt in zip(texts_pred, texts_gt)])\n",
    "\n",
    "        loss = fnn.ctc_loss(log_probs=log_probs,  # (T, N, C)\n",
    "                            targets=seqs_gt,  # N, S or sum(target_lengths)\n",
    "                            input_lengths=seq_lens_pred,  # N\n",
    "                            target_lengths=seq_lens_gt)  # N\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "    print(f'Train {i + 1}, {np.mean(epoch_losses)}')\n",
    "    print(f'Train {i + 1} Levenstein, {np.mean(levensteint_losses)}')\n",
    "    writer.add_scalar('Recognition/Train/loss', np.mean(epoch_losses), i)\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    epoch_losses = []\n",
    "    levensteint_losses = []\n",
    "    crnn.eval()\n",
    "    for j, b in enumerate(tqdm.tqdm(val_dataloader, total=len(val_dataloader))):\n",
    "        images = b[\"image\"].to(device)\n",
    "        seqs_gt = b[\"seq\"]\n",
    "        seq_lens_gt = b[\"seq_len\"]\n",
    "\n",
    "        seqs_pred = crnn(images).cpu()\n",
    "        log_probs = fnn.log_softmax(seqs_pred, dim=2)\n",
    "        seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
    "        \n",
    "        texts_pred = decode(seqs_pred, crnn.alphabet)\n",
    "        texts_gt = b[\"text\"]\n",
    "        levensteint_losses.extend([distance(pred, gt) for pred, gt in zip(texts_pred, texts_gt)])\n",
    "\n",
    "        loss = fnn.ctc_loss(log_probs=log_probs,  # (T, N, C)\n",
    "                            targets=seqs_gt,  # N, S or sum(target_lengths)\n",
    "                            input_lengths=seq_lens_pred,  # N\n",
    "                            target_lengths=seq_lens_gt)  # N\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        if best_loss > epoch_losses[-1]:\n",
    "            best_loss = epoch_losses[-1]\n",
    "            with open(f'{experiment_name}.pth', 'wb') as fp:\n",
    "                torch.save(crnn.state_dict(), fp)\n",
    "        \n",
    "    lr_scheduler.step(np.mean(levensteint_losses))\n",
    "    print(f'Valid {i + 1}, {np.mean(epoch_losses)}')\n",
    "    print(f'Valid {i + 1} Levenstein, {np.mean(levensteint_losses)}')\n",
    "    writer.add_scalar('Recognition/Valid/loss', np.mean(epoch_losses), i)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "with open(f'{experiment_name}.pth', 'rb') as fp:\n",
    "    state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "crnn = CRNN(rnn_bidirectional=True)\n",
    "crnn.load_state_dict(state_dict)\n",
    "crnn.to(device)\n",
    "crnn.eval()\n",
    "print('Model loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
